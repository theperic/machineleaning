library(ggplot2)
library(rattle)
library(caret)
train_data<-read.csv("machine learning/MachLearnAssignment/pml-training.csv")
train_data<-read.csv("pml-training.csv")
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
```{r, cache=TRUE}
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
head(training[,1:10])
dim(train_data)
train_data<-train_data[-1 , ]
dim(train_data)
head(training[,1:10])
dim(train_data)
train_data<-train_data[ ,-1 ]
dim(train_data)
```
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,-1 ]
dim(train_data)
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
dim(testing)
forest<-train(classe ~., data=testing, method="rf", ntree=100,prox=TRUE)
head(train_data[,1:10])
inTrain<-createDataPartition(y=train_data$classe, p=.95, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=testing, method="rf", ntree=10,prox=TRUE)
