library(ggplot2)
library(rattle)
library(caret)
train_data<-read.csv("machine learning/MachLearnAssignment/pml-training.csv")
train_data<-read.csv("pml-training.csv")
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
```{r, cache=TRUE}
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
head(training[,1:10])
dim(train_data)
train_data<-train_data[-1 , ]
dim(train_data)
head(training[,1:10])
dim(train_data)
train_data<-train_data[ ,-1 ]
dim(train_data)
```
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,-1 ]
dim(train_data)
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
dim(testing)
forest<-train(classe ~., data=testing, method="rf", ntree=100,prox=TRUE)
head(train_data[,1:10])
inTrain<-createDataPartition(y=train_data$classe, p=.95, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=testing, method="rf", ntree=10,prox=TRUE)
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
library(ggplot2)
library(rattle)
library(caret)
train_data<-read.csv("pml-training.csv")
output: html_document
---
After loading the data I removed all columns contianing null values.  This left 93 co
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4) ]
dim(train_data)
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest2<-train(classe ~., data=training, method="rf", ntree=20,prox=TRUE)
```
forest2<-train(classe ~., data=training, method="rf", ntree=5,prox=TRUE)
head(training[,1:10])
head(training[,3:10])
```{r, cache=TRUE}
for(i in ncol(train_data)-1){
train_data[,i]<-as.numberic(train_data[,i])
}
for(i in ncol(train_data)-1){
train_data[,i]<-as.numeric(train_data[,i])
}
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
inTrain<-createDataPartition(y=train_data$classe, p=.05, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest2<-train(classe ~., data=training, method="rf", ntree=5,prox=TRUE)
forest3<-train(classe ~., data=training, method="rf", ntree=30,prox=TRUE)
pred<-predict(forest3,testing)
training$predRight<-pred==testing$classe
forest3<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
train_data<-read.csv("pml-training.csv")
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
train_data<-train_data[ ,c(-1, -2,-3,-4) ]
head(train_data[1:10])
head(train_data[4:10])
head(train_data[3:10])
class(train_data[3:10])
class(train_data[,3:10])
class(train_data[3,3:10])
class(train_data[3,3])
class(train_data[3,4])
class(train_data[3,5])
for(i in c(3:(ncol(train_data)-1){
for(i in c(3:(ncol(train_data)-1)){
i<-0
num_cols<-ncol(train_data)-1
for(i in c(3:num_cols)){
i
class(train_data[5,i])
}
i<-0
num_cols<-ncol(train_data)-1
s<-ncol(train_data)-1
nfor(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
for(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
}
for(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
forest3<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
pred<-predict(forest3,testing)
training$predRight<-pred==testing$classe
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
forest4<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest4,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
forest5<-train(classe ~., data=training, method="rf", ntree=15,prox=TRUE)
pred5<-predict(forest5,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
pred5<-predict(forest5,testing)
testing$predRight<-pred5==testing$classe
table(pred5,testing$classe)
?predict
head(pred)
pred
summary(pred)
inTrain<-createDataPartition(y=train_data$classe, p=.10, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest6<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest6,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
head(training[,1:6])
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
dim(train_data)
head(training[,1:6])
head(train_data[,1:6])
train_data<-read.csv("pml-training.csv")
head(train_data[,1:6])
head(train_data[,1:8])
head(train_data[,1:9])
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
head(train_data[,1:9])
dim(train_data)
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
d
head(train_data[,1:10])
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
head(train_data[,1:10])
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
head(train_data[,1:10])
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
head(train_data[,1:10])
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
head(train_data[,1:10])
dim(train_data)
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
}
inTrain<-createDataPartition(y=train_data$classe, p=.05, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=testing, method="rf", ntree=3,prox=TRUE)
forest<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
pred<-predict(forest4,testing)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-testing$predRight/len(pred)
dim(pred)
dim(testing$classe)
length(testing$classe)
length(pred)
correct<-testing$predRight/length(pred)
correct
correct<-length(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
length(testing$predRight)
length(pred)
head(pred)
head(testing$predRight)
sum(testing$predRight)
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
forest<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
forest<-train(classe ~., data=training, method="rf", ntree=50,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=50,prox=TRUE)
inTrain<-createDataPartition(y=train_data$classe, p=.25, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
