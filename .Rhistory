train_data<-train_data[ ,-1 ]
dim(train_data)
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
dim(testing)
forest<-train(classe ~., data=testing, method="rf", ntree=100,prox=TRUE)
head(train_data[,1:10])
inTrain<-createDataPartition(y=train_data$classe, p=.95, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=testing, method="rf", ntree=10,prox=TRUE)
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
library(ggplot2)
library(rattle)
library(caret)
train_data<-read.csv("pml-training.csv")
output: html_document
---
After loading the data I removed all columns contianing null values.  This left 93 co
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4) ]
dim(train_data)
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest2<-train(classe ~., data=training, method="rf", ntree=20,prox=TRUE)
```
forest2<-train(classe ~., data=training, method="rf", ntree=5,prox=TRUE)
head(training[,1:10])
head(training[,3:10])
```{r, cache=TRUE}
for(i in ncol(train_data)-1){
train_data[,i]<-as.numberic(train_data[,i])
}
for(i in ncol(train_data)-1){
train_data[,i]<-as.numeric(train_data[,i])
}
inTrain<-createDataPartition(y=train_data$classe, p=.20, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
inTrain<-createDataPartition(y=train_data$classe, p=.05, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest2<-train(classe ~., data=training, method="rf", ntree=5,prox=TRUE)
forest3<-train(classe ~., data=training, method="rf", ntree=30,prox=TRUE)
pred<-predict(forest3,testing)
training$predRight<-pred==testing$classe
forest3<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
train_data<-read.csv("pml-training.csv")
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
train_data<-train_data[ ,c(-1, -2,-3,-4) ]
head(train_data[1:10])
head(train_data[4:10])
head(train_data[3:10])
class(train_data[3:10])
class(train_data[,3:10])
class(train_data[3,3:10])
class(train_data[3,3])
class(train_data[3,4])
class(train_data[3,5])
for(i in c(3:(ncol(train_data)-1){
for(i in c(3:(ncol(train_data)-1)){
i<-0
num_cols<-ncol(train_data)-1
for(i in c(3:num_cols)){
i
class(train_data[5,i])
}
i<-0
num_cols<-ncol(train_data)-1
s<-ncol(train_data)-1
nfor(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
for(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
}
for(i in c(3:num_cols)){
print(i)
print(class(train_data[5,i]))
}
forest3<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
pred<-predict(forest3,testing)
training$predRight<-pred==testing$classe
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
forest4<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest4,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
forest5<-train(classe ~., data=training, method="rf", ntree=15,prox=TRUE)
pred5<-predict(forest5,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
pred5<-predict(forest5,testing)
testing$predRight<-pred5==testing$classe
table(pred5,testing$classe)
?predict
head(pred)
pred
summary(pred)
inTrain<-createDataPartition(y=train_data$classe, p=.10, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest6<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest6,testing)
testing$predRight<-pred==testing$classe
table(pred,testing$classe)
head(training[,1:6])
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
dim(train_data)
head(training[,1:6])
head(train_data[,1:6])
train_data<-read.csv("pml-training.csv")
head(train_data[,1:6])
head(train_data[,1:8])
head(train_data[,1:9])
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
head(train_data[,1:9])
dim(train_data)
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
d
head(train_data[,1:10])
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
head(train_data[,1:10])
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7,-8) ]
head(train_data[,1:10])
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
## drop columns with NAs
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
head(train_data[,1:10])
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
head(train_data[,1:10])
dim(train_data)
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
}
inTrain<-createDataPartition(y=train_data$classe, p=.05, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=testing, method="rf", ntree=3,prox=TRUE)
forest<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
pred<-predict(forest4,testing)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-testing$predRight/len(pred)
dim(pred)
dim(testing$classe)
length(testing$classe)
length(pred)
correct<-testing$predRight/length(pred)
correct
correct<-length(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
length(testing$predRight)
length(pred)
head(pred)
head(testing$predRight)
sum(testing$predRight)
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
forest<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
forest<-train(classe ~., data=training, method="rf", ntree=50,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
inTrain<-createDataPartition(y=train_data$classe, p=.75, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=50,prox=TRUE)
inTrain<-createDataPartition(y=train_data$classe, p=.25, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
library(ggplot2)
library(rattle)
library(caret)
train_data<-read.csv("pml-training.csv")
##    test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
head(train_data[,1:10])
dim(train_data)
## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
dim(train_data)
#     i<-0
#     num_cols<-ncol(train_data)-1
#     for(i in c(1:num_cols)){
#         print(i)
#         print(class(train_data[5,i]))
#     }
```
```{r, cache=TRUE}
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
}
```
I allocated 75% of the data to my training set and reserved 25% for cross validation.
```{r, cache=TRUE}
## Create training set split
inTrain<-createDataPartition(y=train_data$classe, p=.15, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=3,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
forest<-train(classe ~., data=training, method="rf", ntree=6,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
dim(train_data)
forest<-train(classe ~., data=training, method="rf", ntree=10,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
I allocated 75% of the data to my training set and reserved 25% for cross validation.
forest<-train(classe ~., data=training, method="rf", ntree=50,prox=TRUE)
```
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
inTrain<-createDataPartition(y=train_data$classe, p=.05, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=training, method="rf", ntree=10,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
inTrain<-createDataPartition(y=train_data$classe, p=.60, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
After testing linear and rPart models I selected the Random Forest model for my analysis.
```{r, cache=TRUE}
forest<-train(classe ~., data=training, method="rf", ntree=10,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
test_data<-read.csv("machine learning/MachLearnAssignment/pml-testing.csv")
test_data<-read.csv("pml-testing.csv")
pred_from_test<-predict(forest,test_data)
names<-colnames(training_data)
```
names<-colnames(train_data)
names
pred_from_test<-predict(forest,test_data[,names])
```
pred_from_test<-predict(forest,test_data[,names])
dim(test_data)
training
training
test_data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(test_data)
dim(train_data)
test_data[,x]
test_data[,"x"]
colnames(test_data)
subset(test_data, select = names)
?subset
test_data[,22]
any(is.na(test_data[,22])
)
any(is.na(test_data[,])
)
any(is.na(test_data)
)
apply(test_data,2, any(is.na(test_data))
)
apply( train_data , 2 , function(x) any(is.na(x)) )
!apply( train_data , 2 , function(x) any(is.na(x)) )
dim(train_data)
read
read
train_data<-read.csv("pml-training.csv")
!apply( train_data , 2 , function(x) any(is.na(x)) )
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(test_data)
! apply( test_data , 2 , function(x) any(is.na(x)) )
dim(test_data)
dim(train_data)
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(train_data)
! apply( test_data , 2 , function(x) any(is.na(x)) )
,c(-1, -2,-3,-4, -5,-6,-7)
,c(-1, -2,-3,-4, -5,-6,-7)
test_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
pred_from_test<-predict(forest,test_data)
pred_from_test<-predict(forest,test_data)
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(train_data[,i])
}
pred_from_test<-predict(forest,test_data)
pred_from_test
test_data
dim(test_data)
train_data<-read.csv("pml-training.csv")
test_data<-read.csv("pml-testing.csv")
## drop columns with NAs in the training data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
for(i in c(3:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(test_data[,i])
}
pred_from_test<-predict(forest,test_data)
dim(test_data)
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
test_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
for(i in c(1:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(test_data[,i])
}
for(i in c(1:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(test_data[,i])
}
dim(test_data)
train_data<-read.csv("pml-training.csv")
test_data<-read.csv("pml-testing.csv")
## drop columns with NAs in the training data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
dim(test_data)
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
test_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
dim(test_data)
dim(train_data)
train_data<-read.csv("pml-training.csv")
test_data<-read.csv("pml-testing.csv")
## drop columns with NAs in the training data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
! apply( test_data , 2 , function(x) any(is.na(x)) )
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
test_data<-test_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
dim(train_data)
dim(test_data)
for(i in c(1:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(test_data[,i])
}
pred_from_test<-predict(forest,test_data)
forest2<-forest
csv
csv
csv
train_data<-read.csv("pml-training.csv")
test_data<-read.csv("pml-testing.csv")
## drop columns with NAs in the training data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
test_data<-test_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
```
train_data<-read.csv("pml-training.csv")
test_data<-read.csv("pml-testing.csv")
## drop columns with NAs in the training data
test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
train_data<-train_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
test_data<-test_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
dim(test_data)
train_data
train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
test_data<-test_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
for(i in c(1:(ncol(train_data)-1))){
train_data[,i]<-as.numeric(train_data[,i])
test_data[,i]<-as.numeric(test_data[,i])
}
train_data$classe
inTrain<-createDataPartition(y=train_data$classe, p=.15, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
```
forest<-train(classe ~., data=training, method="rf", ntree=10,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
pred_from_test<-predict(forest,test_data)
pred_from_test
inTrain<-createDataPartition(y=train_data$classe, p=.6, list=FALSE)
training<-train_data[inTrain,]
testing<-train_data[-inTrain,]
forest<-train(classe ~., data=training, method="rf", ntree=10,prox=TRUE)
pred<-predict(forest,testing)
testing$predRight<-pred==testing$classe
correct<-sum(testing$predRight)/length(pred)
correct
table(pred,testing$classe)
dim(train)
dim(training)
pred_from_test<-predict(forest,test_data)
pred_from_test
class(pred_from_test)
as.character(pred_from_test)
output<-as.character(pred_from_test)
output
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
answers<-as.character(pred_from_test)
pml_write_files = function(x){
pml_write_files(answers)
}
answers<-as.character(pred_from_test)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
time0<-system.time()
forest3<-train(classe ~., data=training2, method="rf", ntree=3,prox=TRUE)
time1<-system.time()
forest6<-train(classe ~., data=training2, method="rf", ntree=6,prox=TRUE)
time2<-system.time()
forest8<-train(classe ~., data=training2, method="rf", ntree=8,prox=TRUE)
time3<-system.time()
forest25<-train(classe ~., data=training2, method="rf", ntree=25,prox=TRUE)
time4<-system.time()
inTrain2<-createDataPartition(y=train_data$classe, p=.1, list=FALSE)
training2<-train_data[inTrain,]
testing2<-train_data[-inTrain,]
time0<-system.time()
forest3<-train(classe ~., data=training2, method="rf", ntree=3,prox=TRUE)
time1<-system.time()
forest6<-train(classe ~., data=training2, method="rf", ntree=6,prox=TRUE)
start_time<-system.time()
start_time
system.time()
dim(train)
dim(training)
