---
title: "Machine Learning Assignment 1"
author: "Eric Larsen"
date: "Friday, February 13, 2015"
output: html_document
---
```{r, cache=TRUE}
    library(caret)
```
#Data Preparation

After loading the data I removed all columns contianing null values, first columns containing nulls in the training data, then applying the same technique for nulls in the test data.  This left 53 columns in my data set.  

```{r, cache=TRUE}
    

    train_data<-read.csv("pml-training.csv")
    test_data<-read.csv("pml-testing.csv")

## drop columns with NAs in the training data
    test_data<-test_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
    train_data<-train_data[ , ! apply( train_data , 2 , function(x) any(is.na(x)) ) ]
    
    train_data<-train_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
    test_data<-test_data[ , ! apply( test_data , 2 , function(x) any(is.na(x)) ) ]
    

```

After preliminary data analysis I excluded the x variable, the subject names, the times stamps, and the window descritions as the tests seemed to be ordered by the class, thus these variable provides a correlation we cannot expect in future data sets.
```{r, cache=TRUE}
    ## Remove the X column as this is an idex that happens to be highly correlated to the outcomes due to sorting.
    train_data<-train_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
    test_data<-test_data[ ,c(-1, -2,-3,-4, -5,-6,-7) ]
    

```

    
I also noticed that several numberic data points were stored as factors, so I converted the data set to numerics

```{r, cache=TRUE}
    for(i in c(1:(ncol(train_data)-1))){
        train_data[,i]<-as.numeric(train_data[,i])
        test_data[,i]<-as.numeric(test_data[,i])
    }
    
```

#Partitioning

I allocated 60% of the data to my training set and reserved 40% for cross validation.

```{r, cache=TRUE}
## Create training set split
    inTrain<-createDataPartition(y=train_data$classe, p=.6, list=FALSE)
    training<-train_data[inTrain,]
    testing<-train_data[-inTrain,]


```

#Model Building

After testing linear and rPart models I selected the Random Forest model for my analysis. I have read that the number of trees required is optimized at when set to the square root of the vaiables. Here after removing incomplete variables I had 53 remaining, requiring approximately 8 trees.   

```{r, cache=TRUE}
    forest<-train(classe ~., data=training, method="rf", ntree=8,prox=TRUE)


    ```


#Expected Error rate

Based on the cross validation of running the prediction against the partitioned section of the training data file, I expect the out of sample error rate to be less than 2% based on the cross validation below demonstrating over 98% correct results in my out of sample test:

```{r}
    pred<-predict(forest,testing)
    testing$predRight<-pred==testing$classe
    correct<-sum(testing$predRight)/length(pred)
    correct

```


And the confusion matrix is as follows:
```{r}
    table(pred,testing$classe)

```

These results confirm that limiting to 8 trees was sufficient for a good predictive model.  I did test with more trees, up to 50, and found only marginal improvement in the results with greatly increased compute time.

For the test data file I ran my final model against the test data with the following result:

```{r}
    pred_from_test<-predict(forest,test_data)
    answers<-as.character(pred_from_test)
    answers
```

#Generating the Evaluation Files

Finally, I output the results into separate files for upload.

```{r}

    ## I am using the instructors code for creating my output files.

    pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
            filename = paste0("problem_id_",i,".txt")
            write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
    }


pml_write_files(answers)

```

